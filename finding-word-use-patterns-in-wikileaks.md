Title: Finding Word Use Patterns in Wikileaks Cables
Date: 2012-06-12 11:10
Slug: finding-word-use-patterns-in-wikileaks
Modified: 2014-01-07 15:36
Status: published
Category: 
Tags: text mining,cables,word cloud,ggplot,wordcloud,unclassified,government,foreign service,wikileaks,classified,R,diplomatic,nlp,natural language processing,diplomatic cables


<div class='post'>
6/18: A follow-up to this post is now available <a href=http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html>here</a>.<br/> <br /><b>Recent Discoveries</b><br /><br />When I was a diplomat, I was always interested in the Wikileaks cables and what could be done with them. Unfortunately, I never got a chance to look at the site in depth, due to security policies. Now that the ex- is firmly prepended to diplomat in my resume, I think that I am finally ready to take that step. <br />I recently realized that the wikileaks cables are available in a handy .sql file online. This of course allowed me to download all 250,000 and import them into a database table (I used psql and the /i command). <br />If you are interested in obtaining the cables for yourself, you will need to download the torrent from <a href="http://file.wikileaks.org/torrent/cable_db_full.7z.torrent">here</a>.<br />Let me just clarify here that I will not be printing the text of any of these cables (which has been done in several newspapers), and that I will not be using any data that is not readily publicly available online.<!--more--><br /><br /><b>That's great, but what can we do with them?</b><br /><br />After I had the cables, I brainstormed to see what I could actually do with them that would be interesting. I came up with a few ideas:<br /><ol><li>Find how topics have changed over time. <ul><li>It's reasonable to assume that the focus of the cables would have shifted from “Soviet Union” this and “USSR” that to the Middle East.</li></ul></li><li>Find out what words typify State Department writers. <ul><li>Anyone who has read cables knows that while they are (mostly) in English, its a strange kind of English.</li></ul></li><li>Find out what words/topics typify secret/classified vs unclassified cables. <ul><li>What topics are more likely to be classified? Does word choice change in classified vs unclassified cables?</li></ul></li></ol>I will get into these topics and more as we continue on through this post.<br /><br /><b>Starting to work with the data</b><br /><br />The first thing we need to do is read the data from a database. I interfaced with my PostgreSQL database via ODBC.<br /><br /><pre><code class="r">channel <- odbcConnect(db_name, uid = "", pwd = "")</code></pre><pre><code class="r"> </code></pre>Now, let's get all the cables from 2010 onwards:<br /><br /><pre><code class="r">cable_frame <- sqlQuery(channel, "SELECT * from cable WHERE date > '2010-01-01'", <br /> stringsAsFactors = FALSE, errors = TRUE)</code></pre><pre><code class="r"> </code></pre>We can make a plot of which senders sent the most cables from 2010 onwards:<br /><br /><pre><code class="r">last_10 <- tail(sort(table(cable_frame$origin)), 10)<br />qplot(names(last_10), last_10, geom = "bar") + opts(axis.title.x = theme_blank()) + <br /> opts(axis.title.y = theme_blank()) + opts(axis.text.x = theme_text(size = 8))</code><div class="separator" style="clear: both; text-align: center;"><br /><a href="http://4.bp.blogspot.com/-DSL_1sfbs8I/T9eXJyytdbI/AAAAAAAAAJo/OsFzWmmHGvw/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-DSL_1sfbs8I/T9eXJyytdbI/AAAAAAAAAJo/OsFzWmmHGvw/s1600/chart.png" /></a></div><br /><code class="r"> </code></pre><br />We can see that the Secretary of State and Embassy Baghdad are the two biggest offenders.<br /><br />Now, we can get all of the cables in the database and see how cable traffic changed over time (or perhaps Wikileaks had a biased sample):<br /><br /><pre><code class="r">all_cables <- sqlQuery(channel, "SELECT * from cable", stringsAsFactors = FALSE, <br /> errors = TRUE)</code></pre><pre><code class="r"> date_tab <- table(as.POSIXlt(all_cables$date)$year + 1900)<br />qplot(names(date_tab), as.numeric(date_tab), geom = "bar") + opts(axis.title.x = theme_blank()) + <br /> opts(axis.title.y = theme_blank()) + opts(axis.text.x = theme_text(size = 8))</code><code class="r"> </code><div class="separator" style="clear: both; text-align: center;"><br /><a href="http://2.bp.blogspot.com/-wWYebnBy87c/T9eXSLGuNQI/AAAAAAAAAJw/avRCWMpn1rc/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-wWYebnBy87c/T9eXSLGuNQI/AAAAAAAAAJw/avRCWMpn1rc/s1600/chart.png" /></a></div><br /><code class="r"> </code></pre><br />The amount of cables rises almost exponentially from 2000 until 2009. I'm assuming that only some of the cables for 2010 were leaked, explaining the low count there.<br /><br />We can get rid of the all_cables file, as we won't need it going forward:<br /><pre><code class="r">rm(all_cables)<br />gc()</code></pre><pre><code class="r"> </code></pre><b>Comparing word usage in the 80's and 90's to word usage today</b><br /><br />Now, we can get to something interesting: we can compare how word usage/topics shifted from 1980-1995 to today. Because there are relatively few cables from early on, we have to specify a 15 year range, which nets us only around 675 cables.<br /><br /><pre><code class="r">cable_present <- sqlQuery(channel, "SELECT * from cable WHERE date > '2010-02-15'", <br /> stringsAsFactors = FALSE, errors = TRUE)</code></pre><pre><code class="r"> cable_past <- sqlQuery(channel, "SELECT * from cable WHERE date > '1980-01-01' AND date < '1995-01-01'", <br /> stringsAsFactors = FALSE, errors = TRUE)</code></pre><pre><code class="r"> </code></pre>Now, we have two challenges. The cables all have line breaks and returns (\r and \n), and a lot of the older cables are in all caps. We will get rid of these issues by removing the breaks/returns and converting everything to all lower case.<br /><br /><pre><code class="r">ppatterns <- c("\\n", "\\r")<br />combined <- tolower(gsub(paste("(", paste(ppatterns, collapse = "|"), <br /> ")", sep = ""), "", c(cable_past$content, cable_present$content)))</code></pre><pre><code class="r"> </code></pre>Now, we can construct a term document matrix which counts the number of times each term occurs in each document:<br /><br /><pre><code class="r">corpus <- Corpus(VectorSource(combined))<br />corpus <- tm_map(corpus, stripWhitespace)<br />cable_mat <- as.matrix(TermDocumentMatrix(corpus, control = list(weighting = weightTf, <br /> removePunctuation = TRUE, removeNumbers = TRUE, wordLengths = c(4, 15))))<br />cable_mat <- cable_mat[rowSums(cable_mat) > 3, ]</code></pre><pre><code class="r"> </code></pre>We remove any words that are under 4 characters or over 15 characters, and additionally remove any terms that appear less than 3 times in the whole group of cables.<br /><br />For convenience, we can split the matrix into one containing past cables and one containing current cables:<br /><br /><pre><code class="r">present_mat <- cable_mat[, (nrow(cable_past) + 1):ncol(cable_mat)]<br />past_mat <- cable_mat[, 1:nrow(cable_past)]<br />rm(cable_mat)<br />gc()</code></pre><pre><code class="r"> </code></pre>Now we can get to the good stuff and find differential word usage between the two sets of cables:<br /><br /><pre><code class="r">chisq_vals <- chisq(rowSums(past_mat), ncol(past_mat) * 100, rowSums(present_mat), <br /> ncol(present_mat) * 100)<br />chisq_direction <- rep(-1, length(chisq_vals))<br />mean_frame <- data.frame(past_mean = rowSums(past_mat)/ncol(past_mat), <br /> present_mean = rowSums(present_mat)/ncol(present_mat))<br />chisq_direction[mean_frame[, 2] > mean_frame[, 1]] <- 1<br />chisq_vals <- chisq_vals * chisq_direction<br />cloud_frame <- data.frame(word = rownames(present_mat), chisq = chisq_vals, <br /> past_sum = rowSums(past_mat), present_sum = rowSums(present_mat))<br />pal <- brewer.pal(9, "Set1")</code></pre><pre><code class="r"> </code></pre>The above code will calculate the statistical difference (chisq) between the terms in the first set of cables (1980-1995), and the second set (cables from february 2010).<br /><br />Now we can make some word clouds. This first cloud contains words that appear in the 2010 cables in a more significant way than in the 1980-1995 cables. A larger size indicates that it more significantly appears in the 2010 cables:<br /><br /><pre><code class="r">wordcloud(cloud_frame$word, cloud_frame$chisq, scale = c(8, 0.3), <br /> min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal, <br /> vfont = c("sans serif", "plain"))<br /></code></pre><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-B0lmUAwHLd4/T9eXk8kWDiI/AAAAAAAAAJ4/DFj78-BhKSM/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-B0lmUAwHLd4/T9eXk8kWDiI/AAAAAAAAAJ4/DFj78-BhKSM/s1600/chart.png" /></a></div><br /><br />This second cloud indicates the words that appear in a significant way in the 1980-1995 cables, but not in the 2010 cables:<br /><br /><pre><code class="r">wordcloud(cloud_frame$word, -cloud_frame$chisq, scale = c(8, 0.3), <br /> min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal, <br /> vfont = c("sans serif", "plain"))</code><div class="separator" style="clear: both; text-align: center;"><br /><a href="http://2.bp.blogspot.com/-Bs8aANHfRz8/T9eXv1E_HzI/AAAAAAAAAKA/wIOCy6v71nk/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-Bs8aANHfRz8/T9eXv1E_HzI/AAAAAAAAAKA/wIOCy6v71nk/s1600/chart.png" /></a></div><br /><code class="r"> </code></pre><br />As we can see, february is very significant in the first plot, which is to be expected, because all of the cables are from february. But, we can also see interesting patterns, like trafficking becoming very important in 2010 vs 1980-1995, and words like development and training gaining prominence. In the second plot, we see more interest in topics like zagreb, soviet, saudi, and croatia.<br /><br /><b>Find out what words typify secret/classified cables vs unclassified in 2010</b><br /><br />Let's take a look at what words/topics are more prevalent in secret or classified cables. Let's first look at how many cables of each type are in our cable_present data frame:<br /><br /><pre><code class="r">table(cable_present$classification)<br /></code></pre><pre><code>## <br />## CONFIDENTIAL CONFIDENTIAL//NOFORN <br />## 719 67 <br />## SECRET SECRET//NOFORN <br />## 188 51 <br />## UNCLASSIFIED UNCLASSIFIED//FOR OFFICIAL USE ONLY <br />## 643 756 <br /></code></pre><br />Now, we will do something similar to what we did above, where the data was split into 2 chunks and the words in each chunk were compared to generate clouds. I have made the code generic by changing the names to set one and set two.<br /><br /><pre><code class="r">cable_set_one <- cable_present[cable_present$classification %in% <br /> c("SECRET", "SECRET//NOFORN"), ]<br />cable_set_two <- cable_present[cable_present$classification %in% <br /> c("UNCLASSIFIED", "UNCLASSIFIED//FOR OFFICIAL USE ONLY"), ]<br />ppatterns <- c("\\n", "\\r")<br />combined <- tolower(gsub(paste("(", paste(ppatterns, collapse = "|"), <br /> ")", sep = ""), "", c(cable_set_one$content, cable_set_two$content)))<br />corpus <- Corpus(VectorSource(combined))<br />corpus <- tm_map(corpus, stripWhitespace)<br />cable_mat <- as.matrix(TermDocumentMatrix(corpus, control = list(weighting = weightTf, <br /> removePunctuation = TRUE, removeNumbers = TRUE, wordLengths = c(4, 15))))<br />cable_mat <- cable_mat[rowSums(cable_mat) > 3, ]<br />one_mat <- cable_mat[, 1:nrow(cable_set_one)]<br />two_mat <- cable_mat[, (nrow(cable_set_one) + 1):ncol(cable_mat)]<br />rm(cable_mat)<br />gc()<br />chisq_vals <- chisq(rowSums(one_mat), ncol(one_mat) * 100, rowSums(two_mat), <br /> ncol(two_mat) * 100)<br />chisq_direction <- rep(-1, length(chisq_vals))<br />mean_frame <- data.frame(one_mean = rowSums(one_mat)/ncol(one_mat), <br /> two_mean = rowSums(two_mat)/ncol(two_mat))<br />chisq_direction[mean_frame[, 2] > mean_frame[, 1]] <- 1<br />chisq_vals <- chisq_vals * chisq_direction<br />cloud_frame <- data.frame(word = rownames(one_mat), chisq = chisq_vals, <br /> one_sum = rowSums(one_mat), two_sum = rowSums(two_mat))<br />pal <- brewer.pal(9, "Set1")</code></pre><pre><code class="r"> </code></pre>We are now ready to plot these new word clouds. Here are words that are typical of set 1 (secret cables) that separate it from set 2 (unclassified cables):<br /><br /><pre><code class="r">wordcloud(cloud_frame$word, -cloud_frame$chisq, scale = c(8, 0.3), <br /> min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal, <br /> vfont = c("sans serif", "plain"))<br /></code></pre><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-_ZDNy9V_mgc/T9eX2FLc4zI/AAAAAAAAAKI/Sa9EAyAYypg/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-_ZDNy9V_mgc/T9eX2FLc4zI/AAAAAAAAAKI/Sa9EAyAYypg/s1600/chart.png" /></a></div><br /><br />And here are words that are typical of set 2 (unclassified cables) that separate it from set 1 (secret cables) :<br /><br /><pre><code class="r">wordcloud(cloud_frame$word, cloud_frame$chisq, scale = c(8, 0.3), <br /> min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal, <br /> vfont = c("sans serif", "plain"))<br /></code></pre><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-BXRsR4qrDRg/T9eX8oV7qDI/AAAAAAAAAKQ/z6cjtDp2WAk/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-BXRsR4qrDRg/T9eX8oV7qDI/AAAAAAAAAKQ/z6cjtDp2WAk/s1600/chart.png" /></a></div><br /><br />This makes sense, as the first cloud has words like icbms and bombers, whereas the second has words like labor and victims, which would be typical of the trafficking in persons/human rights reports.<br /><br /><b>Find out what words typify secret/classified cables vs unclassified from 1960-2000</b><br /><br />Now, we can look at what words differentiated secret cables from unclassified cables from 1960 to 2000.<br /><br />Here is the cloud that shows what words appear significantly in the secret cables, but not in the unclassified cables:<br /><br /><pre><code class="r">wordcloud(cloud_frame$word, -cloud_frame$chisq, scale = c(8, 0.3), <br /> min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal, <br /> vfont = c("sans serif", "plain"))<br /></code></pre><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-UJTOmCdJLfU/T9eYCEJyakI/AAAAAAAAAKY/1qI4WEMx0M0/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-UJTOmCdJLfU/T9eYCEJyakI/AAAAAAAAAKY/1qI4WEMx0M0/s1600/chart.png" /></a></div><br /><br />And here is the cloud that shows what words appear significantly in the unclassified cables, but not in the secret cables:<br /><br /><pre><code class="r">wordcloud(cloud_frame$word, cloud_frame$chisq, scale = c(8, 0.3), <br /> min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal, <br /> vfont = c("sans serif", "plain"))<br /></code></pre><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-CRQ8Fl1-MTQ/T9eYIjgT3eI/AAAAAAAAAKg/WJjEtDeTM7E/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-CRQ8Fl1-MTQ/T9eYIjgT3eI/AAAAAAAAAKg/WJjEtDeTM7E/s1600/chart.png" /></a></div><br /><br /><b>Conclusion</b><br /><br />It's very interesting to see how these patterns change over time. Particularly, seeing what the classified topics were from 1960-2000 versus unclassified is interesting. I really wanted to see how State Department writers differ from normal english writers, but I don't have the time to do it right now. It will have to wait for the next post.</div>